=== Ceph

==== 安装
.配置基础yum源
[source,bash]
----
yum clean all
rm -rf /etc/yum.repos.d/*.repo
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
sed -i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo
sed -i '/aliyuncs/d' /etc/yum.repos.d/epel.repo
----

.配置ceph源
[source,bash]
----
cat >> /etc/yum.repos.d/ceph.repo << EOF
[ceph]
name=ceph
baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/
gpgcheck=0
priority =1
[ceph-noarch]
name=cephnoarch
baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/
gpgcheck=0
priority =1
[ceph-source]
name=Ceph source packages
baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/SRPMS
gpgcheck=0
priority=1
EOF
----


.更新源
----
yum clean all && yum makecache
----


.安装Ceph
----
yum install ceph -y
----

.安装主节点 ceph-deploy
----
yum install ceph-deploy -y
----

.三个节点都部署mon
ceph-deploy new node1 node2 node3

==== 配置
.增加以下内容到 /root/cluster/ceph.conf
[source,bash]
----
public_network=192.168.102.0/24
osd_pool_default_size = 2
mon_pg_warn_max_per_osd=20000
----

.配置初始 monitor(s)、并收集所有密钥
----
ceph-deploy mon create-initial
----

.把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点
ceph-deploy admin node1 node2 node3

.查看ceph状态
----
ceph -s
----


[NOTE]
各主机添加一块硬盘用以挂载

.node1
[source,bash]
----
mkfs.xfs /dev/sdb

mkdir -p /var/local/osd0

mount /dev/sdb /var/local/osd0/
----


.node2
[source,bash]
----
mkfs.xfs /dev/sdb

mkdir -p /var/local/osd1

mount /dev/sdb /var/local/osd1/
----

.node3
[source,bash]
----
mkfs.xfs /dev/sdb

mkdir -p /var/local/osd2

mount /dev/sdb /var/local/osd2/
----

.创建osd
----
ceph-deploy osd prepare node1:/var/local/osd0 node2:/var/local/osd1 node3:/var/local/osd2
----

.激活osd
----
ceph-deploy osd activate node1:/var/local/osd0 node2:/var/local/osd1 node3:/var/local/osd2
----

.查看状态
----
ceph-deploy osd list node1 node2 node3
----

各节点修改ceph.client.admin.keyring权限
----
chmod +r /etc/ceph/ceph.client.admin.keyring
----

.部署mds
----
ceph-deploy mds create node1 node2 node3
----

.查看状态
----
ceph mds stat
----

.设置存储池                        PG
[source,bash]
----
ceph osd pool create cephfs_data 128

ceph osd pool create cephfs_metadata 128

ceph fs new cephfs cephfs_metadata cephfs_data

ceph mon stat
----

.创建挂载目录
----
mkdir /ceph/cephfs -pv
----


.挂载启用 cephx 认证的 Ceph 文件系统，你必须指定用户名、密钥（共享目录，全部节点挂载）
----
mount -t ceph node1:6789,node2:6789,node3:6789:/ /ceph/cephfs -o name=admin,secret=AQBcVANiuORJDBAAkrK9yHFtDGQlAlaZ/ycc1g==
----

.卸载
----
umount /ceph/cephfs
----


.ceph-fuse客户端挂载
----
yum -y install ceph-fuse

ceph-fuse -m node1:6789,node2:6789,node3:6789 /ceph/cephfs/
----


==== osd节点宕机重新加入集群
.设置osd状态为out
----
ceph osd out osd.0
----

.从集群中删除
----
ceph osd rm 0
----

.从crush中删除
----
ceph osd crush remove osd.0
----

.清理认证信息
----
ceph auth del osd.0

ceph osd rm 0
----


.卸载磁盘
----
umount /var/local/osd0
----

.清理磁盘
----
rm -rf /var/local/osd0/*
----
